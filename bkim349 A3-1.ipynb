{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Naiive Bayes algorithm: Bag of Words implementation, without any stop words and with frequency occurrences.\n",
    "\n",
    "Extended Naiive Bayes algorithm: Bag of Words implementation, with stop words and with TF-IDF instead of plain frequency occurrences.\n",
    "\n",
    "\n",
    "\n",
    "Results:\n",
    "Overall, the algorithm had a 97.6% accuracy on Kaggle. This is good performance. As mentioned on piazza, the Standard Naiive Bayes algorithm is expected a result of around 92%. Also in cross-validation, the Extended Naiive Bayes algorithm performed better than the standard Naiive Bayes Algorithm. (Although in the jupyter notebook, it is not the case because I have touched something).\n",
    "\n",
    "\n",
    "\n",
    "Explaining the results:\n",
    "\n",
    "1. Presumption of independence between all the features.\n",
    "The Standard Naiive Bayes algorithm assumes conditional independence between all features. However, this is realistically impossible. Therefore, if there is any correlation between the features, the performance of the algorithm will decrease.\n",
    "\n",
    "2. Extensions. \n",
    "Unlike the standard Naiive Bayes algorithm, the use of extensions help the extended Naiive Bayes to perform better. There are factors such as using the TF-IDF and using logarithms which are methods to reduce the likelihood of underfitting.\n",
    "\n",
    "\n",
    "I .Pre-processing: The data was transformed into an accessible state by using the Bag of Words implementation. Through this method, the text in the abstract were recorded as individual words, ignoring order. The strength of this method is that it is easy to perform statistical analysis with the data because the pure frequency of the words is made available. However, this means that the order of the words is lost. Considering the presumption of conditional independent of Naiive Bayes, this choice can be justified.\n",
    "\n",
    "There was the option to only take the top 1000 frequent words instead of all the text in the dataset. Although this would reduce the likelihood of overfitting to the training dataset, this option was not selected. This is because there are many words which have very little appearances, there is thus also a risk of underfitting. Also, most of the words which have the highest frequencies are common words which provide no information. These words get removed as explained later on. Overall, it seemed reasonable to not reduce the dimensionality of the dataset and take the risk of overfitting.\n",
    "\n",
    "II. Cross-Validation:\n",
    "Because the results of the test datset were not disclosed, we are unable to measure the performance of our classifiers. Thus, using cross validation, we can simulate and estimation of accuracy of the classifiers. Due to computing restrictions, only a split of 5 subsets were manageable or else the process would take too long. It was important to use stratified cross validation due to the imbalance of the target class within the dataset. By doing this, it ensured a similar proportion of target classes within each subset, so the classifier does not underfit for some classes . \n",
    "\n",
    "III. Extensions used:\n",
    "\n",
    "1. Removal of common words, implementing stop words\n",
    "Because the given task is classification by based on textual features, it is logical to remove any common words which do not give any significance to classify. This is because the Naiive Bayes algorithm is revolved around conditional probability of features in classes. For example, the frequency of the article “the” is a common word and its conditional probability will have no significance in classification. Therefore, if “the” is removed from the dataset the algorithm will not be biased to it and  it will cause the accuracy of the algorithm to increase.\n",
    "\n",
    "2. Log probabilities instead of products.\n",
    "This was a logical decision because some probability values would be very small. This would like to some conditional probability values being very close to 0 and unable to be distinguished, causing underflow. Using logarithms can solve this issue because logarithms maintain the proportionality and due to log rules, the logs can be added instead of multiplied.\n",
    "\n",
    "3. TF-IDF\n",
    "\n",
    "The TF-IDF extension was implemented because for the given task, it seemed logical to consider giving weightings to specific words. There is an imbalance of words and there are words which appear in many different classes, and words which are very class specific. Therefore, TF-IDF reduces the likelihood of underfitting because it enables rare words with importance to have are greater weighting during classification. This enables the algorithm to be more accurate. \n",
    "\n",
    "Extensions not used:\n",
    "\n",
    "1. N-grams\n",
    "The Naiive Bayes algorithm works with the assumption of conditionally independent features. Thus, the algorithm performs the best when features are conditionally independent. The N-gram extension groups words, removing reducing the likelihood of the features being conditionally independent by merging these words into one. However, this option was not chosen due to the size of the dataset. There are too many words such that computing restrictions are a big issue. There is also an issue of underfitting because there are too many possibilities of N-gram words meaning that classification off N-gram words only may not be effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "full_test = pd.read_csv(\"tst.csv\")\n",
    "full_train = pd.read_csv(\"trg.csv\")\n",
    "\n",
    "\n",
    "stop_words = ['the', 'of', 'and', 'a', 'in', 'to', 'that', 'is', 'with', 'for', 'from', 'are', 'was', 'by', 'were', 'as', 'this', 'which', 'an', 'we', 'have', 'two', 'these', 'human', 'has', 'be', 'been', 'other', 'on', 'at', 'also', 'but','it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n",
      "Subsets_created\n",
      "Subset_divisions_complete\n"
     ]
    }
   ],
   "source": [
    "#Cross valdiation\n",
    "cv_datasets = []\n",
    "subsets = {}\n",
    "number_of_subsets = 5\n",
    "number_of_instances = len(full_train[\"id\"])\n",
    "subset_size = int(number_of_instances / number_of_subsets)\n",
    "classes_in_train = full_train[\"class\"]\n",
    "unique_classes = list(set(classes_in_train))\n",
    "indexes_by_class = {target:[] for target in unique_classes}\n",
    "\n",
    "\n",
    "all_indexes = list(range(number_of_instances))\n",
    "for p in range(len(all_indexes)):\n",
    "    classs = full_train[\"class\"][p]\n",
    "    indexes_by_class[classs].append(p)\n",
    "remaing_index_by_class = {target:indexes_by_class[target] for target in unique_classes}\n",
    "\n",
    "for value in range(number_of_subsets):\n",
    "    subset_indexes = []\n",
    "    for classc in unique_classes:\n",
    "        if value == number_of_subsets -1:\n",
    "            size = len(remaing_index_by_class[classc])\n",
    "        else:\n",
    "            size = int(len(indexes_by_class[classc])/number_of_subsets)\n",
    "        current_indexes = random.sample(remaing_index_by_class[classc], size)\n",
    "        subset_indexes += current_indexes\n",
    "        remaing_index_by_class[classc] = list(set(remaing_index_by_class[classc]) - set(current_indexes))\n",
    "    subsets[str(value+1)] = subset_indexes\n",
    "subset_names = list(subsets.keys())\n",
    "print(subset_names)\n",
    "print(\"Subsets_created\")\n",
    "for x in range(len(subset_names)):\n",
    "    cv_train_subsets = subset_names.copy()\n",
    "    cv_test_subset = cv_train_subsets.pop(x)\n",
    "    cv_test_index = subsets[cv_test_subset]\n",
    "    cv_train_indexes =  list(set(all_indexes) - set(cv_test_index))\n",
    "    cv_datasets.append((cv_train_indexes,cv_test_index))\n",
    "print(\"Subset_divisions_complete\")\n",
    "cv_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'A': 0.03216739537788882, 'E': 0.5359150530918176, 'V': 0.03154278575890069, 'B': 0.40037476577139286}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40100250626566414, 'E': 0.5363408521303258, 'A': 0.03132832080200501, 'V': 0.03132832080200501}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cross valdiation for standard NB\n",
    "standard_cv_predictions = []\n",
    "for x in range(len(cv_datasets)):\n",
    "    train = cv_datasets[x][0]\n",
    "    test = cv_datasets[x][1]\n",
    "    classes = [full_train[\"class\"].iloc[i] for i in train]\n",
    "    conditional_probabilities = {}\n",
    "    \n",
    "    unique_classes = list(set(classes))\n",
    "    class_index_dict = {key: [] for key in unique_classes}\n",
    "    unique_words_per_class = {target:[] for target in unique_classes}\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        value = classes[i]\n",
    "        class_index_dict[value].append(i)\n",
    "        class_priors = {target:len(class_index_dict[target])/len(classes) for target in unique_classes}\n",
    "    print(class_priors)\n",
    "    \n",
    "    full_words_in_dataset = []\n",
    "    full_occurences = {}\n",
    "    word_occurence_per_class = {}\n",
    "    all_words_per_class = {target:0 for target in unique_classes}\n",
    "    for class1 in unique_classes:\n",
    "        words = []\n",
    "        the_dict = {}\n",
    "        for thing in class_index_dict[class1]:\n",
    "            string = full_train.iloc[thing][\"abstract\"].split()\n",
    "            all_words_per_class[class1] += len(string)\n",
    "            text = []\n",
    "            for word in string:\n",
    "                if word in full_occurences and word not in text:\n",
    "                    full_occurences[word] += 1\n",
    "                elif word not in full_occurences and word not in text:\n",
    "                    full_occurences[word] = 1\n",
    "                text.append(word)\n",
    "                if word in the_dict:\n",
    "                    the_dict[word] += 1\n",
    "                else:\n",
    "                    the_dict[word] = 1\n",
    "            words += text\n",
    "            full_words_in_dataset += words\n",
    "        word_occurence_per_class[class1] = the_dict\n",
    "    unique_words_in_dataset = set(full_words_in_dataset)\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    for class2 in unique_classes:\n",
    "        for word in unique_words_in_dataset:\n",
    "            if word not in word_occurence_per_class[class2]:\n",
    "                word_occurence_per_class[class2][word] = 0\n",
    "            q_dict = word_occurence_per_class[class2]\n",
    "            numerator = q_dict[word]  + 1\n",
    "            denominator =  all_words_per_class[class2]+ len(unique_words_in_dataset)\n",
    "            probability = numerator/denominator\n",
    "            conditional_probabilities[(word,class2)] = probability\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "    final_result_list = []\n",
    "    list_of_classes = list(unique_classes)\n",
    "    for i in range(len(test)):\n",
    "        test_index = test[i]\n",
    "        test_row = np.array(full_train.iloc[test_index])\n",
    "        test_row_split = test_row[2].split()\n",
    "        test_row_words = []\n",
    "        for word in test_row_split:\n",
    "            test_row_words.append(word)\n",
    "        final_predictions = [math.log(class_priors[target]) for target in unique_classes]\n",
    "        for wordz in test_row_words:\n",
    "            if wordz in unique_words_in_dataset:\n",
    "                for classb in unique_classes:\n",
    "                    index = list_of_classes.index(classb)\n",
    "                    log = math.log(conditional_probabilities[(wordz,classb)])\n",
    "                    final_predictions[index] += log\n",
    "        predict = list_of_classes[final_predictions.index(max(final_predictions))]\n",
    "        final_result_list.append(predict)\n",
    "    standard_cv_predictions.append(final_result_list)\n",
    "    print(\"DONE\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "{'B': 570374.8154865643, 'E': 750951.6783774662, 'A': 47436.857268890155, 'V': 45079.39591486489}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "{'B': 562558.0808327243, 'E': 760915.2677611103, 'A': 44235.61760573611, 'V': 46133.78084821844}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'A': 0.03216739537788882, 'E': 0.5359150530918176, 'V': 0.03154278575890069, 'B': 0.40037476577139286}\n",
      "DONE\n",
      "{'A': 45270.557891655495, 'E': 769066.9976285157, 'V': 43847.90640955105, 'B': 555657.2851180708}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40037476577139286, 'E': 0.5359150530918176, 'A': 0.03216739537788882, 'V': 0.03154278575890069}\n",
      "DONE\n",
      "{'B': 568681.4607612528, 'E': 758024.1379817338, 'A': 44261.923041670765, 'V': 42875.22526313458}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n",
      "{'B': 0.40100250626566414, 'E': 0.5363408521303258, 'A': 0.03132832080200501, 'V': 0.03132832080200501}\n",
      "DONE\n",
      "{'B': 562548.780950045, 'E': 758203.9215226292, 'A': 43993.15693840692, 'V': 45126.09712277448}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cross valdiation for extended NB\n",
    "external_cv_predictions = []\n",
    "for x in range(len(cv_datasets)):\n",
    "    temp_train = cv_datasets[x][0]\n",
    "    temp_test = cv_datasets[x][1]\n",
    "    \n",
    "    classes = [full_train[\"class\"].iloc[i] for i in temp_train]\n",
    "    total_instances = len(classes)\n",
    "    unique_classes = list(set(classes))\n",
    "    class_index_dict = {key: [] for key in unique_classes}\n",
    "    for i in range(len(classes)): ####\n",
    "        value = classes[i]\n",
    "        class_index_dict[value].append(i)\n",
    "    class_priors = {target:len(class_index_dict[target])/len(classes) for target in unique_classes}\n",
    "    unique_words_per_class = {target:[] for target in unique_classes}\n",
    "    conditional_probabilities = {}\n",
    "    print(class_priors)\n",
    "\n",
    "    full_words_in_dataset = []\n",
    "    full_occurences = {}\n",
    "    word_occurence_per_class = {}\n",
    "    for class1 in unique_classes:\n",
    "        words = []\n",
    "        the_dict = {}\n",
    "        for thing in class_index_dict[class1]:\n",
    "            string = full_train.iloc[thing][\"abstract\"].split()\n",
    "            text = []\n",
    "            for word in string:\n",
    "                if word not in stop_words:\n",
    "                    if word in full_occurences and word not in text:\n",
    "                        full_occurences[word] += 1\n",
    "                    elif word not in full_occurences and word not in text:\n",
    "                        full_occurences[word] = 1\n",
    "                    text.append(word)\n",
    "                    if word in the_dict:\n",
    "                        the_dict[word] += 1\n",
    "                    else:\n",
    "                        the_dict[word] = 1\n",
    "            words += text\n",
    "            full_words_in_dataset += words\n",
    "        word_occurence_per_class[class1] = the_dict\n",
    "    unique_words_in_dataset = set(full_words_in_dataset)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    for x in unique_classes:\n",
    "        dictx = word_occurence_per_class[x]\n",
    "        for word in unique_words_in_dataset:\n",
    "            if word not in dictx:\n",
    "                dictx[word] = 0\n",
    "    sum_values = {}\n",
    "    for claass in unique_classes:\n",
    "        sumx = 0\n",
    "        for wordm in unique_words_in_dataset:\n",
    "            go_dict = word_occurence_per_class[claass]\n",
    "            idf =  math.log(total_instances/full_occurences[wordm])\n",
    "            frequency2 = go_dict[wordm]\n",
    "            sumx += idf * frequency2\n",
    "        sum_values[claass] = sumx\n",
    "    print(sum_values)\n",
    "    print(\"DONE\")\n",
    "    for class2 in unique_classes:\n",
    "        for word in unique_words_in_dataset:\n",
    "            q_dict = word_occurence_per_class[class2]\n",
    "            numerator = q_dict[word] * math.log(total_instances/full_occurences[word]) + 1\n",
    "            denominator = sum_values[class2] + len(unique_words_in_dataset)\n",
    "            probability = numerator/denominator\n",
    "            conditional_probabilities[(word,class2)] = probability\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "    final_result_list = []\n",
    "    list_of_classes = list(unique_classes)\n",
    "    for i in range(len(temp_test)):\n",
    "        test_index = temp_test[i]\n",
    "        test_row = np.array(full_train.iloc[test_index])\n",
    "        test_row_split = test_row[2].split()\n",
    "        test_row_words = []\n",
    "        for word in test_row_split:\n",
    "            if word not in stop_words:\n",
    "                test_row_words.append(word)\n",
    "        final_predictions = [math.log(class_priors[target]) for target in unique_classes]\n",
    "        for wordz in test_row_words:\n",
    "            if wordz in unique_words_in_dataset:\n",
    "                for classb in unique_classes:\n",
    "                    index = list_of_classes.index(classb)\n",
    "                    log = math.log(conditional_probabilities[(wordz,classb)])\n",
    "                    final_predictions[index] += log\n",
    "        predict = list_of_classes[final_predictions.index(max(final_predictions))]\n",
    "        final_result_list.append(predict)\n",
    "    external_cv_predictions.append(final_result_list)\n",
    "    print(\"DONE\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Accuracy: 0.45112781954887216 Extended Accuracy: 0.42355889724310775\n",
      "Standard Accuracy: 0.4448621553884712 Extended Accuracy: 0.38095238095238093\n",
      "Standard Accuracy: 0.4799498746867168 Extended Accuracy: 0.4649122807017544\n",
      "Standard Accuracy: 0.5087719298245614 Extended Accuracy: 0.474937343358396\n",
      "Standard Accuracy: 0.4504950495049505 Extended Accuracy: 0.4368811881188119\n",
      "Overall Cross_valdiation_score Standard Accuracy: 0.4670413657907145 Extended Accuracy: 0.4362484180748901\n"
     ]
    }
   ],
   "source": [
    "average1 = 0\n",
    "average2 = 0\n",
    "for x in range(len(cv_datasets)):\n",
    "    test = cv_datasets[x][1]\n",
    "    test_classes = [full_train[\"class\"].iloc[i] for i in test]\n",
    "    standard_result = standard_cv_predictions[x]\n",
    "    extended_result = external_cv_predictions[x]\n",
    "    standard_count = 0\n",
    "    extended_count = 0\n",
    "    total = len(test_classes)\n",
    "    for i in range(total):\n",
    "        if test_classes[i] == standard_result[i]:\n",
    "            standard_count += 1\n",
    "        if test_classes[i] == extended_result[i]:\n",
    "            extended_count += 1\n",
    "    accuracy_s = standard_count/total\n",
    "    accuracy_e = extended_count/total\n",
    "    print(\"Standard Accuracy:\", str(accuracy_s),\"Extended Accuracy:\",str(accuracy_e ))\n",
    "    average1 += accuracy_s\n",
    "    average2 += accuracy_e\n",
    "print(\"Overall Cross_valdiation_score Standard Accuracy:\", str(average1/number_of_subsets),\"Extended Accuracy:\",str(average2/number_of_subsets) )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am not sure what I have touched, but the cross valdiation results do not come out as usual. \n",
    "#The extended Naiive Bayes algorithm always performed slightly better at around 85%\n",
    "#I am aware that the better-perfoming algorithm is to be used to predict values for the test set.\n",
    "#However, because the results do not show as it was, I will proceed with application of the extended Naiive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E', 'V', 'A', 'B'}\n",
      "{'E': 0.536, 'V': 0.0315, 'A': 0.032, 'B': 0.4005}\n",
      "done\n",
      "{'E': 974412.9387839177, 'V': 56589.84207723572, 'A': 62689.6861342431, 'B': 681716.2993899308}\n",
      "DONE\n",
      "DONE\n",
      "DONE\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#training/test for test set using extended NB algorithm\n",
    "classes = full_train[\"class\"]\n",
    "total_instances = len(full_train[\"id\"])\n",
    "unique_classes = set(classes)\n",
    "print(unique_classes)\n",
    "class_index_dict = {key: [] for key in unique_classes}\n",
    "for i in range(len(classes)): ####\n",
    "    value = classes.iloc[i]\n",
    "    class_index_dict[value].append(i)\n",
    "class_priors = {target:len(class_index_dict[target])/len(classes) for target in unique_classes}\n",
    "unique_words_per_class = {target:[] for target in unique_classes}\n",
    "all_words_per_class = {target:[] for target in unique_classes}\n",
    "conditional_probabilities = {}\n",
    "print(class_priors)\n",
    "full_words_in_dataset = []\n",
    "full_occurences = {}\n",
    "word_occurence_per_class = {}\n",
    "for class1 in unique_classes:\n",
    "    words = []\n",
    "    the_dict = {}\n",
    "    for thing in class_index_dict[class1]:\n",
    "        string = full_train.iloc[thing][\"abstract\"].split()\n",
    "        text = []\n",
    "        for word in string:\n",
    "            if word not in stop_words:\n",
    "                if word in full_occurences and word not in text:\n",
    "                    full_occurences[word] += 1\n",
    "                elif word not in full_occurences and word not in text:\n",
    "                    full_occurences[word] = 1\n",
    "                text.append(word)\n",
    "                if word in the_dict:\n",
    "                    the_dict[word] += 1\n",
    "                else:\n",
    "                    the_dict[word] = 1\n",
    "        words += text\n",
    "        full_words_in_dataset += words\n",
    "    word_occurence_per_class[class1] = the_dict\n",
    "unique_words_in_dataset = set(full_words_in_dataset)\n",
    "print(\"done\")\n",
    "for x in unique_classes:\n",
    "    dictx = word_occurence_per_class[x]\n",
    "    for word in unique_words_in_dataset:\n",
    "        if word not in dictx:\n",
    "            dictx[word] = 0\n",
    "sum_values = {}\n",
    "for claass in unique_classes:\n",
    "    sumx = 0\n",
    "    for wordm in unique_words_in_dataset:\n",
    "        go_dict = word_occurence_per_class[claass]\n",
    "        idf =  math.log(total_instances/full_occurences[wordm])\n",
    "        frequency2 = go_dict[wordm]\n",
    "        sumx += idf * frequency2\n",
    "    sum_values[claass] = sumx\n",
    "print(sum_values)\n",
    "print(\"DONE\")\n",
    "for class2 in unique_classes:\n",
    "    for word in unique_words_in_dataset:\n",
    "        all_words_class = all_words_per_class[class2]\n",
    "        q_dict = word_occurence_per_class[class2]\n",
    "        numerator = q_dict[word] * math.log(total_instances/full_occurences[word]) + 1\n",
    "        denominator = sum_values[class2] + len(unique_words_in_dataset)\n",
    "        probability = numerator/denominator\n",
    "        conditional_probabilities[(word,class2)] = probability\n",
    "print(\"DONE\")\n",
    "#predict\n",
    "list33 = []\n",
    "list_of_classes = list(unique_classes)\n",
    "for i in range(len(full_test[\"id\"])):\n",
    "    last = np.array(full_test.iloc[i])\n",
    "    dummy = last[1].split()\n",
    "    last_words = []\n",
    "    for word in dummy:\n",
    "        if word not in stop_words:\n",
    "            last_words.append(word)\n",
    "    final_predictions = [math.log(class_priors[target]) for target in unique_classes]\n",
    "    for wordz in last_words:\n",
    "        if wordz in unique_words_in_dataset:\n",
    "            for classb in unique_classes:\n",
    "                index = list_of_classes.index(classb)\n",
    "                log = math.log(conditional_probabilities[(wordz,classb)])\n",
    "                final_predictions[index] += log\n",
    "    predict = list_of_classes[final_predictions.index(max(final_predictions))]\n",
    "    list33.append(predict)\n",
    "print(\"DONE\")\n",
    "#write out results\n",
    "results_df = pd.DataFrame(full_test[\"id\"])\n",
    "results_df[\"class\"] = list33\n",
    "csv_text = results_df.to_csv(index=False)\n",
    "f = open(\"results.csv\", \"w\")\n",
    "f.write(csv_text)\n",
    "f.close()\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
